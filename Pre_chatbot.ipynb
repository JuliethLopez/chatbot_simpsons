{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pre-chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliethLopez/chatbot_simpsons/blob/master/Pre_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTODpHwM75a_",
        "colab_type": "text"
      },
      "source": [
        "# Pre-chatbot\n",
        "Este el cuaderno final donde vamos a utilizar los modelos ya entrenados para obtener la respuesta de la maquina. Vamos a cargar todo desde drive porque fué ahí donde se guardaron tanto los modelos como los diccionarios para hacer el preprocesamiento. Además, vamos a unir todo dentro de una sola función para que sea mas sencillo utilizarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkFwcx3gf-Nh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "4be6fd56-a3a2-4874-d795-436c2287b992"
      },
      "source": [
        "# drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkCR-M7XuIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# librerias\n",
        "import pickle\n",
        "import random\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjv5Vf5wLjN",
        "colab_type": "text"
      },
      "source": [
        "## 1.Clasificador\n",
        "### Cargar modelo\n",
        "Aquí descomprimimos el archivo que contiene el modelo clasificador y cargamos el modelo con *tf.keras.models*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ZzR9biyZcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "11f984e2-c1de-4648-98f4-e6e6e87653c1"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/clasificador.zip' #julieth"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Simpsons Chat bot/Modelos guardos/clasificador.zip\n",
            "   creating: clasificador/\n",
            "   creating: clasificador/assets/\n",
            "   creating: clasificador/variables/\n",
            "  inflating: clasificador/variables/variables.data-00000-of-00002  \n",
            "  inflating: clasificador/variables/variables.index  \n",
            "  inflating: clasificador/variables/variables.data-00001-of-00002  \n",
            "  inflating: clasificador/saved_model.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJI4M-33SxM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelo_clasificador = tf.keras.models.load_model('clasificador')\n",
        "#modelo_clasificador.summary() #arquitectura del modelo"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvMZ8ChbDB2a",
        "colab_type": "text"
      },
      "source": [
        "### Cargar para preprocesamiento\n",
        "El modelo recibe las entradas codificadas, por ello, cargamos los trigramas y los bigramas para hacer el preprocesamiento de las entradas del usuario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHa4QuY8TUoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trigramas\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/freqnew.pickle', 'rb') as f:\n",
        "  freqnew = pickle.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME5r0X6znfiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bigramas\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/freqbnew.pickle', 'rb') as f:\n",
        "  freqbnew = pickle.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-DlbbnVzna",
        "colab_type": "text"
      },
      "source": [
        "### Función clasificadora\n",
        "Aquí utilizamos lo que cargamos anteriormente para que todo lo relacionado al modelo clasificador se pueda llamar desde una sola función. Hemos hecho además la función *sequences_to_text* para volver de los valores numericos a la etiqueta en texto porque la función *sequences_to text* de *label_tokenizer* (creado con *Tokenizer* de *tensorflow.keras.preprocessing.text*) parece no funcionar correctamante, ya que con esa función se obtenian de vuelta solo tres de las cuatro etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoHV1PHoDN02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# funcion de frase tokenizada a palabras\n",
        "reverse_word_map = dict(map(reversed, label_tokenizer.word_index.items()))\n",
        "\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Busca palabras en el diccionario\n",
        "    label = [reverse_word_map.get(i) for i in list_of_indices]\n",
        "    return(label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOwD0xgWWEtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clasifica (sentence):\n",
        "  sentence = re.sub(' +', ' ',sentence.lower()) #sentence en minusculas\n",
        "  sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence) #quita signos de puntuación\n",
        "  # preprocesamiento trigramas\n",
        "  vectorizer3 = CountVectorizer(vocabulary=freqnew.keys(), ngram_range=(3,3))\n",
        "  X3 = vectorizer3.fit_transform([sentence])\n",
        "  # preprocesamiento bigramas\n",
        "  vectorizer2 = CountVectorizer(vocabulary=freqbnew.keys(), ngram_range=(2,2))\n",
        "  X2 = vectorizer2.fit_transform([sentence])\n",
        "  # concatenamos\n",
        "  sequences_bt = np.concatenate((X3.toarray(), X2.toarray()), axis=1)\n",
        "  # predicción\n",
        "  pred = np.argmax(modelo_clasificador.predict(sequences_bt), axis=-1) #prediccion del modelo\n",
        "  pred = sequence_to_text(pred) #de tokenizer a label\n",
        "  pred = ''.join(pred)\n",
        "  \n",
        "  return (pred)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuvNedw92aJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ea2713ee-5b9a-474c-962d-17f7086ae549"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Would this be a good time to be honest?\"\n",
        "clasifica(sentence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'marge_simpson'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53XtW8yussz-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "674dc2d0-c193-4939-e429-c698c49135ce"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Manzana\"\n",
        "clasifica(sentence)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'<OOV>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9U4A5Mguty",
        "colab_type": "text"
      },
      "source": [
        "## 2.Generador\n",
        "### Cargar modelo\n",
        "Igual que con el modelo anterior, cargamos todo lo relacionado a este modelo desde drive, tanto el modelo como los diccionarios para el pre y post procesamiento. En este caso tenemos un modelo generador sencillo *modelo_generador*, y otro con un entrenamiento más avanzado *modelo_generador_pers*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70r4vK_QgZBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "7a07d80e-e161-4863-ffa4-5c9efe859511"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador.zip' #julieth"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador.zip\n",
            "   creating: generador/\n",
            "  inflating: generador/saved_model.pb  \n",
            "   creating: generador/variables/\n",
            "  inflating: generador/variables/variables.data-00000-of-00002  \n",
            "  inflating: generador/variables/variables.index  \n",
            "  inflating: generador/variables/variables.data-00001-of-00002  \n",
            "   creating: generador/assets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME7p6QlIgciY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "146d570b-2cfb-473c-bcf4-4b5eeea1dec0"
      },
      "source": [
        "modelo_generador = tf.keras.models.load_model('generador')\n",
        "#modelo_clasificador.summary() #arquitectura del modelo"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1AONJqutfym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "2d689f32-c2a7-4aeb-9f18-869ab3c9e0cc"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador_pers.zip' #julieth"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador_pers.zip\n",
            "   creating: generador_pers/\n",
            "  inflating: generador_pers/saved_model.pb  \n",
            "   creating: generador_pers/assets/\n",
            "   creating: generador_pers/variables/\n",
            "  inflating: generador_pers/variables/variables.index  \n",
            "  inflating: generador_pers/variables/variables.data-00001-of-00002  \n",
            "  inflating: generador_pers/variables/variables.data-00000-of-00002  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-h1WTZetqvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1205dc5f-e166-4119-9bbb-688dfe204d3c"
      },
      "source": [
        "modelo_generador_pers = tf.keras.models.load_model('generador_pers')\n",
        "#modelo_clasificador.summary() #arquitectura del modelo"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MB9G_j7hbKy",
        "colab_type": "text"
      },
      "source": [
        "### Cargar para preprocesamiento\n",
        "Para que el modelo acepte la entretada necesitamos transformar la entrada en texto a numeros, para esto cargamos los diccionarios de carácteres a índices y viceversa, índices a carácteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUO42a3ahanM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# char a id\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/charxid.pickle', 'rb') as f:\n",
        "  charxid = pickle.load(f)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS_PZpc3jWz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#id a char\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/idxchar.pickle', 'rb') as f:\n",
        "  idxchar = pickle.load(f)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck052drUmQc5",
        "colab_type": "text"
      },
      "source": [
        "### Función generadora\n",
        "Al igual que en el modelo anterior, utilizamos todo lo relacionado con el modelo dentro de la función generadora. La función tiene el atributo *model* que permite seleccionar entre el modelo basico *modelo_generador* y el modelo con un entrenamiento azanzado *modelo_generador_pers*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6tNzKcl4ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funcion para generar texto usando el modelo pre-entrenado\n",
        "def generate_text(model,start_string, num_generate):\n",
        "  #vectorizacion\n",
        "  input_eval = [charxid[s] for s in start_string] #vector columna\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #vector fila, agrega una diension\n",
        "  \n",
        "  #guardamos el texto predicho\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "  #tamaño del bache== 1\n",
        "\n",
        "  model.reset_states() #reinia o borra el estado recurrente de la red. Dejando valores aleatorios o ceros.\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      #removemos la dimensión del batch, quita una dimensión\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      #usamos la distribución categorica para predecir la palabra que retorna el modelo\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      #utilizamos la palabra predicha y el estado oculto anterior como entrada\n",
        "      input_eval = tf.expand_dims([predicted_id], 0) #devuelve un tensor con una dimensión adicional en el eje de índice\n",
        "\n",
        "      text_generated.append(idxchar[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhkR6UwY22AL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "88ff45e5-6f9e-4f42-a8c9-ab1a395a1706"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Would this be a good time to be honest?\"\n",
        "generate_text(modelo_generador,sentence, 100)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Would this be a good time to be honest?\\r\\nHomer Simpson: (EVERING ON FLEARIZE) I know you're a limbo. Moe.\\r\\nBart Simpson: Stopper may be hom\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bmMh9K5vF9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d5865ae4-6f0f-4a7e-f3a9-590ed0672d38"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Homer:Would this be a good time to be honest?\"\n",
        "generate_text(modelo_generador,sentence, 100)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Homer:Would this be a good time to be honest?\\r\\nEdna Krabappel-Flanders: But would have a ballant bad you say I've slented myself, but...\\r\\nLenny L\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLIH_DuvCDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ffdafd95-7720-4f10-f53c-aa00da0297fc"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Would this be a good time to be honest?\"\n",
        "generate_text(modelo_generador_pers,sentence, 100)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Would this be a good time to be honest? And Houten: (TO MOE, QUICKLY) Hey, hey! Hey, you know... but he speakers the Karamologea! (FRIGHTEN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NxonfpUVAzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1d080fe4-ef0d-4483-8dd1-5ff2e427c170"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Homer:Would this be a good time to be honest?\"\n",
        "generate_text(modelo_generador_pers,sentence, 100)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Homer:Would this be a good time to be honest?\\r\\nLisa Simpson: ...Mrench Eas Day, this year's pretty assemblation.\\r\\nMarge Simpson: Bart's going to \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-uuNUNMo_I-",
        "colab_type": "text"
      },
      "source": [
        "## 3.Funciones para unir redes\n",
        "La salida de la red clasificadora es una etiqueta, sin embargo esas etiquetas contienen guion bajo, para limpiar la etiqueta utilizamos la función *etiquet*, Luego, para unir la salida de la red clasificadora con la entrada del generador de texto, utilizamos la función *unión*, y para limpiar la salida del generador, usamos la función *output_ultimate* que está contenida dentro de *union*. Además, tenemos una lista de frases de respuesta que se eligen aleatoriamente en caso de que el modelo clasificador no sea capaz de generar una etiqueta, así la maquina entregara una respuesta aleatoria definida previamente en lugar de entrar a la red generadora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PceWweFLqxBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def etiquet(pre_etiqueta):\n",
        "  if pre_etiqueta != '<OOV>':\n",
        "    pre_etiqueta = pre_etiqueta[:pre_etiqueta.index('_')].capitalize()\n",
        "  return (pre_etiqueta)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvtqBWsPMLDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_ultimate(line):\n",
        "  if line.split('\\n')[1] == ' ':\n",
        "    line2=line.split('\\n')[2]\n",
        "    labelsLine2 = line2[:line2.index(':')+1]\n",
        "    line_ultimate = line2.replace(labelsLine2,\"\")\n",
        "    print(line_ultimate)\n",
        "  else:\n",
        "    line2=line.split('\\n')[1]\n",
        "    labelsLine2 = line2[:line2.index(':')+1]\n",
        "    line_ultimate = line2.replace(labelsLine2,\"\")\n",
        "    print(line_ultimate)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cuq01zGKzmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frases de respuesta si no logra clasificar\n",
        "frases_respuesta = [\"Mmm, donuts.\",\n",
        "                      \"Whatever, I'll be at Moe's.\",\n",
        "                      \"Eat my shorts!\",\n",
        "                      \"If anyone wants me, I'll be in my room.\",\n",
        "                      \"Bye!\",\n",
        "                      \"To start press any key. Where's the ANY key?\",\n",
        "                      \"The lesson is, never try\",\n",
        "                      \"I'm normally not a praying man, but if you're up there, please save me, Superman\",\n",
        "                      \"You'll have to speak up I'm wearing a towel\",\n",
        "                      \"I am so smart, S-M-R-T\",\n",
        "                      \"You're making a scene\",\n",
        "                      \"Can I have some money now?\"]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE3Xn48TK5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def union(model, lst1, lst2):\n",
        "  if lst1 != '<OOV>':\n",
        "    final_list = lst1 + \": \" + lst2 + \"\\n\"\n",
        "    final_list = final_list.split('\\n')[0]\n",
        "    final_list = generate_text(model, final_list, 1000)\n",
        "    final_list = output_ultimate(final_list)\n",
        "  else:\n",
        "    a = random.randint(0, 1)\n",
        "    if a == 0:\n",
        "      b = random.randint(0, len(frases_respuesta)) #elige una frase aleatoriamente\n",
        "      final_list = frases_respuesta[b]\n",
        "    else:\n",
        "      final_list = lst2 + \"\\n\"\n",
        "      final_list = final_list.split('\\n')[0]\n",
        "      final_list = generate_text(model, final_list, 1000)\n",
        "      final_list = output_ultimate(final_list)\n",
        "  \n",
        "  return (final_list)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyYD9U_ipz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed91061d-d8e9-4ac1-8d17-cb6bed00186c"
      },
      "source": [
        "#ejemplo\n",
        "union(modelo_generador, 'Marge', \"Oh, my homer, if you knew how much I love you\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Ya-yo-yo!!\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhSh45rOvrPS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6f4b7f6-3fbe-46f3-8ed3-0597492c2de0"
      },
      "source": [
        "#ejemplo\n",
        "union(modelo_generador_pers, 'Marge', \"Oh, my homer, if you knew how much I love you\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Ja we think. (DYING HEADS) Ahh!\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2oKpfy-hr_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34e3f195-f0bf-4fde-e22f-652387ed329d"
      },
      "source": [
        "#ejemplo: entra al generador\n",
        "union('<OOV>', \"Oh, my homer, if you knew how much I love you\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Oh no, what a career?\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbf9mFSOihcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e804fb1e-c8a7-4911-c76d-f71debd4081d"
      },
      "source": [
        "#ejemplo: elige una respuesta pre-establecida\n",
        "union('<OOV>', \"Oh, my homer, if you knew how much I love you\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'I am so smart, S-M-R-T'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAGMYMLXYtFr",
        "colab_type": "text"
      },
      "source": [
        "# 4.Pre chatbot\n",
        "Ahora la función *prechatbot* une todo lo hecho en el repositorio y es posible usarla de una forma sencilla. ¿Como funciona internamente? Cuando el usuario pone una entrada, ésta entrada es transformada a un vector que indica la composición de la entrada con el vocabulario de la red clasificadora, la red clasificadora devuelve una etiqueta que conecta la forma de escribir de un ususario con el estilo de alguno de los personajes prinicipales de los simpsons, hacemos esto porque son los personajes principales quienes tienen mas interaciones con los demas personajes, si la etiqueta es valida, se une esa etiqueta con la entrada del usuario y se genera una respuesta, para generar la respuesta se utiliza bien o el modelo sencillo *modelo_generador* o el modelo con entrenamiento avanzado *modelo_generador_pers* que es definido por el usuario en la entrada, si la etiqueta no es valida, se devuelve aleatoriamente una frase predeterminada, o se genera una respuesta sin la etiqueta del personaje, tomando como entrada para el generador solo la entrada del usuario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1LSBYcyq1ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prechatbot(model,sentence):\n",
        "  pre_etiqueta = clasifica(sentence)\n",
        "  etiqueta = etiquet(pre_etiqueta)\n",
        "  respuesta = union(model, etiqueta, sentence)\n",
        "  return (respuesta)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYK_X0skRTHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "da2213f8-0de2-470d-af50-99680ce74842"
      },
      "source": [
        "#ejemplo\n",
        "Usuario = \"Would this be a good time to be honest?\"\n",
        "print(\"modelo_generador\")\n",
        "prechatbot(modelo_generador, Usuario)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador\n",
            " Well, have for there?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHaP2SJSysYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88c5897e-4741-48a2-a12e-425a24955b80"
      },
      "source": [
        "print(\"modelo_generador_pers\")\n",
        "prechatbot(modelo_generador_pers, Usuario)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador_pers\n",
            " (CONCENTLES) I don't like we hear therefeys for the next drivew beer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJNQ9H-6rAg2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "98dba4a9-63d2-452a-e070-431e6c649e99"
      },
      "source": [
        "#ejemplo\n",
        "Usuario = \"Hello, how are you? Are you fine?\"\n",
        "print(\"modelo_generador\")\n",
        "prechatbot(modelo_generador, Usuario)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador\n",
            " That's unie the cash. It's gotder and fly those plimy in the same it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP99TLDVzJO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aaefc72e-43ce-48fd-e9c5-4b6def87fa88"
      },
      "source": [
        "print(\"modelo_generador_pers\")\n",
        "prechatbot(modelo_generador_pers, Usuario)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador_pers\n",
            " I get off your imagination.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIDfsRxrCeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "07246e38-cb72-4899-e5e1-c0e3009342dd"
      },
      "source": [
        "#ejemplo\n",
        "Usuario = \"Manzana\"\n",
        "print(\"modelo_generador\")\n",
        "prechatbot(modelo_generador, Usuario)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Whatever, I'll be at Moe's.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blrBoL9Ryvoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a933f67-eef8-40a8-90fd-77d8249d3462"
      },
      "source": [
        "print(\"modelo_generador_pers\")\n",
        "prechatbot(modelo_generador_pers, Usuario)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador_pers\n",
            " No!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpEcb2m7rHKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1001bfc2-9997-4f31-8bd2-24dc3e6fd823"
      },
      "source": [
        "#ejemplo\n",
        "Usuario = \"Hello\"\n",
        "print(\"modelo_generador\")\n",
        "prechatbot(modelo_generador, Usuario)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador\n",
            " Absolutely. You have to takep up him I'm are after well.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3QjQ-wTUpBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a555604f-b8d0-4983-f45b-b0ed44946bfd"
      },
      "source": [
        "print(\"modelo_generador_pers\")\n",
        "prechatbot(modelo_generador_pers, Usuario)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelo_generador_pers\n",
            " I told that other seven gear. (THEN) Ux how much siren sideple dollacs. We just outlet you after seeing my teams lost.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l1PcHFQydgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}