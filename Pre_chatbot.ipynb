{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pre-chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliethLopez/chatbot_simpsons/blob/master/Pre_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkFwcx3gf-Nh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "183391c8-f419-4c78-bfff-9029e8bf340a"
      },
      "source": [
        "# drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkCR-M7XuIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# librerias\n",
        "import pickle\n",
        "import re, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjv5Vf5wLjN",
        "colab_type": "text"
      },
      "source": [
        "# 1.Clasificador\n",
        "### Cargar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ZzR9biyZcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "d0933be2-2f11-42ec-ea75-6af5b0d3493d"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/clasificador.zip' #julieth"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Simpsons Chat bot/Modelos guardos/clasificador.zip\n",
            "   creating: clasificador/\n",
            "   creating: clasificador/assets/\n",
            "   creating: clasificador/variables/\n",
            "  inflating: clasificador/variables/variables.data-00000-of-00002  \n",
            "  inflating: clasificador/variables/variables.index  \n",
            "  inflating: clasificador/variables/variables.data-00001-of-00002  \n",
            "  inflating: clasificador/saved_model.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJI4M-33SxM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelo_clasificador = tf.keras.models.load_model('clasificador')\n",
        "#Check its architecture\n",
        "#modelo_clasificador.summary()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvMZ8ChbDB2a",
        "colab_type": "text"
      },
      "source": [
        "### Cargar para preprocesamiento\n",
        "Cargamos el label_tokenizer, el encodeer, los trigramas y bigramas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WdRzWKuD8-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#label tokenizer\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/label_tokenizer.pickle', 'rb') as f:\n",
        "  label_tokenizer = pickle.load(f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2-uIHleDLIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/encoder.pickle', 'rb') as f:\n",
        "  encoder = pickle.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHa4QuY8TUoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trigramas\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/freqnew.pickle', 'rb') as f:\n",
        "  freqnew = pickle.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME5r0X6znfiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bigramas\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/freqbnew.pickle', 'rb') as f:\n",
        "  freqbnew = pickle.load(f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-DlbbnVzna",
        "colab_type": "text"
      },
      "source": [
        "### Función clasificadora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoHV1PHoDN02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# funcion de frase tokenizada a palabras\n",
        "reverse_word_map = dict(map(reversed, label_tokenizer.word_index.items()))\n",
        "\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Busca palabras en el diccionario\n",
        "    label = [reverse_word_map.get(i) for i in list_of_indices]\n",
        "    return(label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOwD0xgWWEtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clasifica (sentence):\n",
        "  sentence = re.sub(' +', ' ',sentence.lower()) #sentence en minusculas\n",
        "  sentence = re.sub('[%s]' % re.escape(string.punctuation), '', sentence) #quita signos de puntuación\n",
        "  # preprocesamiento trigramas\n",
        "  vectorizer3 = CountVectorizer(vocabulary=freqnew.keys(), ngram_range=(3,3))\n",
        "  X3 = vectorizer3.fit_transform([sentence])\n",
        "  # preprocesamiento bigramas\n",
        "  vectorizer2 = CountVectorizer(vocabulary=freqbnew.keys(), ngram_range=(2,2))\n",
        "  X2 = vectorizer2.fit_transform([sentence])\n",
        "  # concatenamos\n",
        "  sequences_bt = np.concatenate((X3.toarray(), X2.toarray()), axis=1)\n",
        "  # predicción\n",
        "  pred = np.argmax(modelo_clasificador.predict(sequences_bt), axis=-1) #prediccion del modelo\n",
        "  pred = encoder.inverse_transform(pred) #de label encoder a tokenizer\n",
        "  pred = sequence_to_text(pred) #de tokenizer a label\n",
        "  \n",
        "  return (pred)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuvNedw92aJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "634a7697-432e-4dd8-b19b-f3068f3c01d6"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Would this be a good time to be honest?\"\n",
        "clasifica(sentence)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['homer_simpson']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9U4A5Mguty",
        "colab_type": "text"
      },
      "source": [
        "# 2.Generador\n",
        "### Cargar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70r4vK_QgZBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "64972b7f-b478-4325-a14a-d81662cb8114"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador.zip' #julieth"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Simpsons Chat bot/Modelos guardos/generador.zip\n",
            "   creating: generador/\n",
            "  inflating: generador/saved_model.pb  \n",
            "   creating: generador/variables/\n",
            "  inflating: generador/variables/variables.data-00000-of-00002  \n",
            "  inflating: generador/variables/variables.index  \n",
            "  inflating: generador/variables/variables.data-00001-of-00002  \n",
            "   creating: generador/assets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME7p6QlIgciY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "62bbb74b-8778-4d79-8aa9-b3d18564a941"
      },
      "source": [
        "modelo_generador = tf.keras.models.load_model('generador')\n",
        "#Check its architecture\n",
        "#modelo_clasificador.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MB9G_j7hbKy",
        "colab_type": "text"
      },
      "source": [
        "### Cargar para preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUO42a3ahanM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# char a id\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/charxid.pickle', 'rb') as f:\n",
        "  charxid = pickle.load(f)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS_PZpc3jWz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#id a char\n",
        "with open('/content/drive/My Drive/Simpsons Chat bot/Modelos guardos/idxchar.pickle', 'rb') as f:\n",
        "  idxchar = pickle.load(f)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck052drUmQc5",
        "colab_type": "text"
      },
      "source": [
        "### Función generadora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6tNzKcl4ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funcion para generar texto usando el modelo pre-entrenado\n",
        "def generate_text(start_string, num_generate):\n",
        "\n",
        "  #convertimos el texto inicial en numeros (vectorizacion)\n",
        "  input_eval = [charxid[s] for s in start_string] #vector columna\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #vector fila, agrega una diension\n",
        "\n",
        "  #guardamos el texto predicho\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  #tamaño del bache== 1\n",
        "  modelo_generador.reset_states() #reinia o borra el estado recurrente de la red. Dejando valores aleatorios o ceros.\n",
        "  for i in range(num_generate):\n",
        "      predictions = modelo_generador(input_eval)\n",
        "      #removemos la dimensión del batch, quita una dimensión\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      #usamos la distribución categorica para predecir la palabra que retorna el modelo\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      #utilizamos la palabra predicha y el estado oculto anterior como entrada\n",
        "      input_eval = tf.expand_dims([predicted_id], 0) #devuelve un tensor con una dimensión adicional en el eje de índice\n",
        "\n",
        "      text_generated.append(idxchar[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhkR6UwY22AL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "1f02f32a-1b0b-40e7-ecac-40e5ab2ee3cc"
      },
      "source": [
        "#ejemplo\n",
        "sentence = \"Homer:Would this be a good time to be honest?\"\n",
        "generate_text(sentence, 1000)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Homer:Would this be a good time to be honest?\\r\\nC. Montgomery Burns: Okayyn: What did you tell my first cant animals? That, you're fluther cake expleaationally-line!\\r\\nMarge Simpson: Ow!\\r\\nHomer Simpson: They're thousands for experienced for I'm the nerd of the old one: The clossbecoming!\\r\\nMarge Simpson: Marge, fine, you're faultive. It's a claum.\\r\\nHomer Simpson: I had wrong?\\r\\nMarge Simpson: (INTRIGUED) And signed Moe Sprinkfoller!\\r\\nBart Simpson: Can't ever come up in there, meet-up Arust Ralph Wait! Yeah!\\r\\nHomer Simpson: How did you like more?\\r\\nComic Book Guy: (CONFUSED) Oh, that's what chapsswere middle they weren't try no pretty. You right with after minthes saying party.\\r\\nBart Simpson: Quite marriage. Yeah, everybody.\\r\\nHomer Simpson: Well, I can let 'em jocks. I wish I am a seems?\\r\\nAnnouncer: Moe name to have to can tell there down there?\\r\\nGOOD DROATH: (READY) Oh hah here not, Angeant.\\r\\nEnnter: (STROMES OF PAUGHTER) Bart, get inside the crazy nuclele 2926, unjterson, no.\\r\\nSeymour Skinner: Marge, the way's the most of War!\\r\\nCapta\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-uuNUNMo_I-",
        "colab_type": "text"
      },
      "source": [
        "#Funciones para unir redes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFkzIhmjq5HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import re "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cuq01zGKzmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frases de respuesta si no logra clasificar\n",
        "frases_respuesta = [\"Mmm, donuts.\",\n",
        "                      \"Whatever, I'll be at Moe's.\",\n",
        "                      \"Eat my shorts!\",\n",
        "                      \"If anyone wants me, I'll be in my room.\",\n",
        "                      \"Bye!\",\n",
        "                      \"To start press any key. Where's the ANY key?\",\n",
        "                      \"The lesson is, never try\",\n",
        "                      \"I'm normally not a praying man, but if you're up there, please save me, Superman\",\n",
        "                      \"You'll have to speak up I'm wearing a towel\",\n",
        "                      \"I am so smart, S-M-R-T\",\n",
        "                    \n",
        "                    \"You're making a scene\",\n",
        "                      \"Can I have some money now?\"]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE3Xn48TK5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def union(lst1, lst2): \n",
        "    if lst1 != \"<OOV>\":\n",
        "        final_list = lst1 + \": \" + lst2 + \"\\n\"\n",
        "        return final_list\n",
        "    else:\n",
        "        a = random.randint(0, len(frases_respuesta)) #elige una frase aleatoriamente\n",
        "        return frases_respuesta[a]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvtqBWsPMLDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_ultimate(line):\n",
        "  if line.split('\\n')[1] == ' ':\n",
        "    line2=line.split('\\n')[2]\n",
        "    labelsLine2 = line2[:line2.index(':')+1]\n",
        "    line_ultimate = line2.replace(labelsLine2,\"\")\n",
        "    print(line_ultimate)\n",
        "  else:\n",
        "    line2=line.split('\\n')[1]\n",
        "    labelsLine2 = line2[:line2.index(':')+1]\n",
        "    line_ultimate = line2.replace(labelsLine2,\"\")\n",
        "    print(line_ultimate)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAGMYMLXYtFr",
        "colab_type": "text"
      },
      "source": [
        "# Pre chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7DydIuRUEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prechatbot(Sentence):\n",
        "  Pre_Etiqueta=clasifica(Sentence)\n",
        "  display_one = ''.join(Pre_Etiqueta)\n",
        "  Etiqueta = display_one[:display_one.index('_')].capitalize()\n",
        "  Transicion = union(Etiqueta, Sentence).split('\\n')[0]\n",
        "  Linea = generate_text(Transicion, 1000)\n",
        "  output_ultimate(Linea)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYK_X0skRTHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe231878-6b78-4e45-f650-9ee62723525d"
      },
      "source": [
        "Usuario = \"Would this be a good time to be honest?\"\n",
        "prechatbot(Usuario)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Pite Door, whick you are the RV down hurt.\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}