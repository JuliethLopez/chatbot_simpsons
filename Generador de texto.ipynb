{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F4lOCtNUtKL",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliethLopez/chatbot_simpsons/blob/master/Generador%20de%20texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_xHJ41-ewD-g"
      },
      "source": [
        "# Chatbot de los Simpsons\n",
        "\n",
        "Con éste código se pretende hacer un chatbot a partir de la generación de texto con un red neuronal recurrente, particularmente una red GRU. El modelo debe aprender a generar caracteres para formar palabras con sentido y con ello una oración que corresponda a una palabra o una frase.\n",
        "\n",
        "Para comenzar cargaremos las librerias y la base de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YOZBjZVYsD4_",
        "outputId": "1ad1add6-9659-42f6-fa34-41ea6201ed45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#librerias\n",
        "import tensorflow as tf #para el uso de keras y el manejo de tensores.\n",
        "import numpy as np #para manejo de bases de datos\n",
        "import os #para leer y escribir archivos\n",
        "import time\n",
        "\n",
        "print(\"Versión de Tensorflow: \", tf.__version__)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Versión de Tensorflow:  2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7p1zzi6QwEAz"
      },
      "source": [
        "La base de datos fue tomada del conjunto de bases de datos __The Simpsons by the Data__ en [data.world](https://data.world/data-society/the-simpsons-by-the-data) (*para usar esta página es necesario registrarse*). Éste contiene distintas bases de datos que con los personajes, las locaciones, los detalles de los episodios, y los dialogos de aproximadamente 600 episodios de los simpsons. Utilizamos la base de datos *simpsons_script_lines.csv* que inicialmente tenia 158271 lineas de conversación, por ejemplo \"Lisa Simpson: Challenge accepted!\" y la filtramos por la variable raw_text=TRUE usando SQL desde la misma página [data.world](https://data.world/data-society/the-simpsons-by-the-data), obteniendo 132113 lineas.\n",
        "\n",
        "    SELECT raw_text\n",
        "    FROM simpsons_script_lines\n",
        "    WHERE speaking_line = 'Guess what. I also play Frankenstein!' OR speaking_line = TRUE\n",
        "    \n",
        "Descargamos el archivo en formato XLSX y lo transformamos a formato TXT, no descargamos los datos en formato CSV porque aparecián caracteres no imprimibles (¦ â € Ã ©)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYPvH81qsD5H",
        "colab": {}
      },
      "source": [
        "#datos\n",
        "text = open('./Datos/simpsons_script_lines.txt', 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qH9fwX7esD5R",
        "outputId": "10dc4da3-31c0-4b98-a309-08721cc774b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "#primeros 200 caracteres\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Miss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\r\n",
            "Lisa Simpson: (NEAR TEARS) Where's Mr. Bergstrom?\r\n",
            "Miss Hoover: I don't know. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlOssfc1wEDZ"
      },
      "source": [
        "Contamos el numero de caracteres porque el modelo se entrenara desde los caracteres, no desde las palabras como veremos más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OTH4ccgsD5M",
        "outputId": "4c44e2cc-717b-4162-f837-2eacc68756e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 9913945 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CelGue5MwEDn"
      },
      "source": [
        "## 1. Vocabulario o alfabeto\n",
        "\n",
        "Si bien los datos están en inglés, hay algunas palabras en otros idiomas debido a la complejitud de los personajes de la serie, por eso hemos decidido dejar los carácteres que pertenecen a esos otro idiomas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rym9-0-gsD5X",
        "outputId": "b958aef8-f416-41af-c977-58da29a8158a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab = sorted(set(text)) #set extrae caracteres distintos, sorted los ordena\n",
        "print ('{} carácteres únicos'.format(len(vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138 carácteres únicos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MMcoUrcmzKWk"
      },
      "source": [
        "Dentro de los caracteres tenemos \\n que se utiliza como indicador para el salto de linea y \\r que se utiliza para regresar del final de la linea al inicio de la linea. Pero por otro lado tenemos un caracter no deseable \\xad que vamos a eliminar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WK28ZFwvzKkA",
        "colab": {}
      },
      "source": [
        "text = text.replace('\\N{SOFT HYPHEN}', '') #elimina \\xad y eliminaria mas caracteres suaves si hubieran."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eDg3nHNl0_AJ",
        "outputId": "b72602d4-d2b6-4629-def2-05c6af09c8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab = sorted(set(text)) #set extrae caracteres distintos, sorted los ordena\n",
        "print ('{} carácteres únicos'.format(len(vocab)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137 carácteres únicos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tr-IbSi_y3vq"
      },
      "source": [
        "## 2. Diccionarios de ida y vuelta\n",
        "Ahora que tenemos el vocabulario de caracteres podemos crear los diccionarios de ida y vuelta que lo que harán es darle un id a cada caracter ya que para la maquina es más facil trabajar con números, de modo que cuando una frase entra, se traduce a numeros cada uno de los caracteres y se predicen los id, que luego seran transformados a carácteres para poder leerlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lXMNM2u8sD5g",
        "colab": {}
      },
      "source": [
        "charxid = {u:i for i, u in enumerate(vocab)} #le damos un indice a cada caractér\n",
        "text_as_int = np.array([charxid[i] for i in text]) #caracteres a números"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1TUMzDuJ6fP1",
        "outputId": "984f63b7-a832-42c2-efc2-56d180347a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "#id de los primeros 10 caracteres\n",
        "for char,_ in zip(charxid, range(10)):\n",
        "    print('{:4s}: {:3d},'.format(repr(char), charxid[char])) #s: str, d: int"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n':   0,\n",
            "'\\r':   1,\n",
            "' ' :   2,\n",
            "'!' :   3,\n",
            "'#' :   4,\n",
            "'$' :   5,\n",
            "'%' :   6,\n",
            "'&' :   7,\n",
            "\"'\" :   8,\n",
            "'(' :   9,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QqdNdncUtM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a21aa068-42e5-403d-88e0-8b2673a24714"
      },
      "source": [
        "#primeros 15 carácteres del texto a enteros\n",
        "print ('{} ---- carácteres a enteros ---- > {}'.format(repr(text[:15]), text_as_int[:15]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Miss Hoover: No' ---- carácteres a enteros ---- > [42 69 79 79  2 37 75 75 82 65 78 27  2 43 75]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ttvBve3jsD50",
        "outputId": "c4d4a967-2589-403d-9f60-707c33932c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "idxchar = np.array(vocab) #números a caracteres\n",
        "print(\"\\nidxchar:\", idxchar)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "idxchar: ['\\n' '\\r' ' ' '!' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0'\n",
            " '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F'\n",
            " 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X'\n",
            " 'Y' 'Z' '[' '\\\\' ']' '_' '`' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k'\n",
            " 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '¡' '¿' 'À'\n",
            " 'Ä' 'È' 'É' 'Ñ' 'Ö' 'Ù' 'Ü' 'à' 'á' 'â' 'ã' 'ä' 'å' 'æ' 'ç' 'è' 'é' 'ê'\n",
            " 'ë' 'ì' 'í' 'ï' 'ñ' 'ò' 'ó' 'ô' 'õ' 'ö' 'ø' 'ù' 'ú' 'û' 'ü' 'ā' 'Ĉ' 'ē'\n",
            " 'ě' 'Ĝ' 'ī' 'Ĭ' 'ł' 'ń' 'ŭ' 'ż' 'ǎ' 'ǐ' 'ǒ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "USt8NcpwwEE3"
      },
      "source": [
        "## 3. Datos de entrenamiento y etiquetas\n",
        "\n",
        "Los datos que tenemos aún no están listos para usarse en el modelo, pues no están etiquetados. Para entender como etiquetar los datos usemos esta frase como ejemplo \"Miss Hoover: No, actually, it was a little of both.\". El modelo que vamos a utilizar necesita una frase de entrada, tomemos \"Miss Hoover: No, actually, it was a littl\" y necesita una etiqueta, en este caso es \"iss Hoover: No, actually, it was a little\", así:\n",
        "\n",
        "\n",
        "|                     Entrada                 |  Etiqueta   |\n",
        "|---------------------------------------------|:---:|\n",
        "| \"Miss Hoover: No, actually, it was a littl\" | \"iss Hoover: No, actually, it was a little\" |\n",
        "| \"iss Hoover: No, actually, it was a little\" | \"ss Hoover: No, actually, it was a little \" |\n",
        "| \"ss Hoover: No, actually, it was a little \" | \"s Hoover: No, actually, it was a little o\" |\n",
        "| \"s Hoover: No, actually, it was a little o\" | \" Hoover: No, actually, it was a little of\" |\n",
        "\n",
        "Entonces, empezaremos por definir el tamañao de las entradas y las etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AyN__8-4sD6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a4a1efa-dbaa-4908-e4be-18e72eb2954a"
      },
      "source": [
        "seq_length = 100 #máxima longitud de la frase\n",
        "#examples_per_epoch = len(text)//(seq_length+1)\n",
        "examples_per_epoch = len(text)-seq_length #numero de datos por epoca\n",
        "examples_per_epoch"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9913834"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0dwAkUXUtNn",
        "colab_type": "text"
      },
      "source": [
        "Los datos necesitan ser transformados a tensores para que puedan ser utilizados en el modelo, los transformaremos con *tf.data.Dataset.from_tensor_slices*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_e7_qjJUtNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c466e59a-7165-453d-8b81-b29b5b4754a1"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #se tranforma de numpy a tf\n",
        "print(\"Tensor:\", char_dataset)\n",
        "\n",
        "#ejemplo\n",
        "for i in char_dataset.take(11):\n",
        "  print(idxchar[i.numpy()])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: <TensorSliceDataset shapes: (), types: tf.int64>\n",
            "M\n",
            "i\n",
            "s\n",
            "s\n",
            " \n",
            "H\n",
            "o\n",
            "o\n",
            "v\n",
            "e\n",
            "r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANMiIEYxFI9I"
      },
      "source": [
        "Ahora podemos construir las secuencias de entrenamiento, tengamos en cuenta que las estas se componen por seq_length caracteres mas un caracter que es el que se usara para crear la etiqueta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_r2QdKtrsD6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "2fafdaee-b737-4324-c45f-4614f0a9d825"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "print(\"sequences:\", sequences)\n",
        "\n",
        "for item in sequences.take(2):\n",
        "  print(repr(idxchar[item.numpy()])) #rep vuelve imprimible lo que es imprimible"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences: <BatchDataset shapes: (101,), types: tf.int64>\n",
            "array(['M', 'i', 's', 's', ' ', 'H', 'o', 'o', 'v', 'e', 'r', ':', ' ',\n",
            "       'N', 'o', ',', ' ', 'a', 'c', 't', 'u', 'a', 'l', 'l', 'y', ',',\n",
            "       ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'l', 'i', 't',\n",
            "       't', 'l', 'e', ' ', 'o', 'f', ' ', 'b', 'o', 't', 'h', '.', ' ',\n",
            "       'S', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', ' ', 'w', 'h', 'e',\n",
            "       'n', ' ', 'a', ' ', 'd', 'i', 's', 'e', 'a', 's', 'e', ' ', 'i',\n",
            "       's', ' ', 'i', 'n', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ',\n",
            "       'm', 'a', 'g', 'a', 'z', 'i', 'n', 'e', 's', ' '], dtype='<U1')\n",
            "array(['a', 'n', 'd', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'n',\n",
            "       'e', 'w', 's', ' ', 's', 'h', 'o', 'w', 's', ',', ' ', 'i', 't',\n",
            "       \"'\", 's', ' ', 'o', 'n', 'l', 'y', ' ', 'n', 'a', 't', 'u', 'r',\n",
            "       'a', 'l', ' ', 't', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 't',\n",
            "       'h', 'i', 'n', 'k', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 'v', 'e',\n",
            "       ' ', 'i', 't', '.', '\\r', '\\n', 'L', 'i', 's', 'a', ' ', 'S', 'i',\n",
            "       'm', 'p', 's', 'o', 'n', ':', ' ', '(', 'N', 'E', 'A', 'R', ' ',\n",
            "       'T', 'E', 'A', 'R', 'S', ')', ' ', 'W', 'h', 'e'], dtype='<U1')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWm4a5UwUtOF",
        "colab_type": "text"
      },
      "source": [
        "Juntemos los caracteres para que sean mas facil de visualizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMzvXkVUtOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f844d1cb-3d2e-42b3-dcc2-b20e62effe10"
      },
      "source": [
        "for item in sequences.take(2):\n",
        "  print(repr(''.join(idxchar[item.numpy()])))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Miss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines '\n",
            "\"and all the news shows, it's only natural that you think you have it.\\r\\nLisa Simpson: (NEAR TEARS) Whe\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke0H4IcsUtOU",
        "colab_type": "text"
      },
      "source": [
        "Aquí dividiremos las secuencias en entradas y etiquetas con la misma longitud. El método *.map* nos permite aplicar una función simple a cada lote, similar a la función apply de R, se busca duplicar cada secuencia y cambiarla para formar el texto de entrada y de salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v1gqMHr1sD6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "368e6b7f-3515-490d-dc82-1d574cd45289"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQE-D13ZsD6s",
        "outputId": "5f783df2-c918-4865-bbb8-7744e907f76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "for item in dataset.take(1):\n",
        "  print(item)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([42, 69, 79, 79,  2, 37, 75, 75, 82, 65, 78, 27,  2, 43, 75, 13,  2,\n",
            "       61, 63, 80, 81, 61, 72, 72, 85, 13,  2, 69, 80,  2, 83, 61, 79,  2,\n",
            "       61,  2, 72, 69, 80, 80, 72, 65,  2, 75, 66,  2, 62, 75, 80, 68, 15,\n",
            "        2, 48, 75, 73, 65, 80, 69, 73, 65, 79,  2, 83, 68, 65, 74,  2, 61,\n",
            "        2, 64, 69, 79, 65, 61, 79, 65,  2, 69, 79,  2, 69, 74,  2, 61, 72,\n",
            "       72,  2, 80, 68, 65,  2, 73, 61, 67, 61, 86, 69, 74, 65, 79])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([69, 79, 79,  2, 37, 75, 75, 82, 65, 78, 27,  2, 43, 75, 13,  2, 61,\n",
            "       63, 80, 81, 61, 72, 72, 85, 13,  2, 69, 80,  2, 83, 61, 79,  2, 61,\n",
            "        2, 72, 69, 80, 80, 72, 65,  2, 75, 66,  2, 62, 75, 80, 68, 15,  2,\n",
            "       48, 75, 73, 65, 80, 69, 73, 65, 79,  2, 83, 68, 65, 74,  2, 61,  2,\n",
            "       64, 69, 79, 65, 61, 79, 65,  2, 69, 79,  2, 69, 74,  2, 61, 72, 72,\n",
            "        2, 80, 68, 65,  2, 73, 61, 67, 61, 86, 69, 74, 65, 79,  2])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjBxK_9-UtO4",
        "colab_type": "text"
      },
      "source": [
        "Juntemos los caracteres para que sean mas facil de visualizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jDk7Y-W1sD6w",
        "outputId": "41f54772-ef57-40bb-fcaf-8353b05e3204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idxchar[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idxchar[target_example.numpy()])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Miss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines'\n",
            "Target data: 'iss Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgeQaGREUtPF",
        "colab_type": "text"
      },
      "source": [
        "Veamos de forma muy corta y más detallada que lo que buscamos con una sola secuencia es:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9E3PSzlCsD6z",
        "outputId": "2588ee23-6eb6-48db-806d-a62d6b6e3f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idxchar[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idxchar[target_idx])))\n",
        "print(\"...\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 42 ('M')\n",
            "  expected output: 69 ('i')\n",
            "Step    1\n",
            "  input: 69 ('i')\n",
            "  expected output: 79 ('s')\n",
            "Step    2\n",
            "  input: 79 ('s')\n",
            "  expected output: 79 ('s')\n",
            "Step    3\n",
            "  input: 79 ('s')\n",
            "  expected output: 2 (' ')\n",
            "Step    4\n",
            "  input: 2 (' ')\n",
            "  expected output: 37 ('H')\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6M4SFfd0wEGh"
      },
      "source": [
        "### Lotes de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q4-fRnFUtPT",
        "colab_type": "text"
      },
      "source": [
        "Vamos a mezclar los datos y los vamos a guardar por lotes de tamaño 64, porque así los datos no tienen dependencia de orden (silimar a la autocorrelación en la regresión) y además hacemos una actualización para cada mini lote de datos. Además las mezclan se harán en un búfer para cada bache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZQ9kWk_sD66",
        "outputId": "6ef12b84-5c25-42f3-e55b-4ba2468759e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "BATCH_SIZE = 64 #tamaño del lote\n",
        "BUFFER_SIZE = 500000 #bufer para mezclar el conjunto de datos\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUCY6AexwEGu"
      },
      "source": [
        "## 4. Construcción del Modelo\n",
        "\n",
        "Para este modelo vamos a utilizar tres capas,\n",
        "   - Embegind: Es un espacio de dimensiones bajas en el que se puede traducir vectores de altas dimensiones. Tambien se considera un mapeo de objetos discretos. El embeding es obtenido internamente a partir concurrencias, proyecciones y otros.\n",
        "   - GRU: (Gated recurrent unit) es un tipo de RNN compuesta por unidades GRU.\n",
        "   - Dense: Todas las neuronas se conectan con todas las neuronas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Zw9jMyJsD6-",
        "colab": {}
      },
      "source": [
        "#Longitud del vocabulario en caracteres, tamaño de entrada(one hot)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#Dimensión de incrustación(embedding), cada caracter sera representado por un vector de 256.\n",
        "embedding_dim = 256\n",
        "\n",
        "#numero de unidades de RNN\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b4R_6GUtPu",
        "colab_type": "text"
      },
      "source": [
        "Entonces, el dataset de entrenamiento tiene lotes de tamaño 64 * 100. Notemos que en la construccion del modelo, en la capa GRU (stateful=TRUE) mantiene la memoria de la secuencia inicial y de los caracteres que van siendo generados, pero solamente se pasa en el primer paso la secuencia de entrada completa. Luego solo se pasa el caracter predicho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ppp0rEoTsD7C",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True, #regresa toda las secuencias\n",
        "                        stateful=True, #cuando se pasa de un batch a otro, no se inicializan las capas intermedias\n",
        "                        recurrent_initializer='glorot_uniform',\n",
        "                        dropout=0.5), #forma de inicialización del estado recurrente\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True, #regresa toda las secuencias\n",
        "                        stateful=True, #cuando se pasa de un batch a otro, no se inicializan las capas intermedias\n",
        "                        recurrent_initializer='glorot_uniform',\n",
        "                        dropout=0.5), #forma de inicialización del estado recurrente\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oqOzws2ZsD7F",
        "colab": {}
      },
      "source": [
        "#modelo\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ZqpLGoNwEID"
      },
      "source": [
        "## 5. Configuración de checkpoints\n",
        "\n",
        "Creamos un directorio *training_checkpoints* y utilizamos *tf.keras.callbacks.ModelCheckpoint* para guardar los puntos de control(los peses)durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ielCBdbsD7l",
        "colab": {}
      },
      "source": [
        "#directorio donde los checkpoints(puntos de control) serán guardados\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "#nombre de los puntos de control(checkpoint files)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, #direccion o ruta\n",
        "    save_weights_only=True) #se guardan solo los pesos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fyTjVqGiwEIK"
      },
      "source": [
        "## 6. Entrenamiento\n",
        "\n",
        "El lote de datos pasa por el modelo y devuelve un tensor de tamaño (64, 100, 65). Como el vocabulario tien 65 caracteres y cada sequencia tiene tamaño 100, el modelo predice el siguiente caracter para cada caracter en la entrada, luego el modelo asigna un valor numérico a cada elemento en el vocabulario. Cómo se elige? Se puede elegir el valor numerico máximo o se puede elegir atoriamente, que resulta ser mejor que tomar el maximo.\n",
        "\n",
        "Para elegir aleatoriamente o tomar la muestra, la distribución es calculada a partir de las predicciones(logits) que son generadas para cada caracter del vocabulario. Para cada lote de 64 secuencias la función de pérdida se obtiene con el promedio de 64∗100=6400 entropías cruzadas dispersas.\n",
        "\n",
        "Para compilar el modelo utilizamos la función de pérdida de entropía cruzada y el optimizardor Adam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "omQ6FoossD7i",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5OHsxhRPsD7u",
        "outputId": "62b0b3e5-a02f-4f29-cca5-fccd155a7c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "#en history queda registrado la funcion de perdida y la precision para el modelo, tanto en entrenamiento como en test\n",
        "EPOCHS=10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.6672\n",
            "Epoch 2/10\n",
            "1533/1533 [==============================] - 248s 162ms/step - loss: 1.2979\n",
            "Epoch 3/10\n",
            "1533/1533 [==============================] - 248s 162ms/step - loss: 1.2499\n",
            "Epoch 4/10\n",
            "1533/1533 [==============================] - 248s 162ms/step - loss: 1.2279\n",
            "Epoch 5/10\n",
            "1533/1533 [==============================] - 248s 162ms/step - loss: 1.2160\n",
            "Epoch 6/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.2083\n",
            "Epoch 7/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.2037\n",
            "Epoch 8/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.2011\n",
            "Epoch 9/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.2001\n",
            "Epoch 10/10\n",
            "1533/1533 [==============================] - 247s 161ms/step - loss: 1.2002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mVbs6nUxwEIV"
      },
      "source": [
        "## 7. Generación de texto\n",
        "\n",
        "### Restauración del último  checkpoint\n",
        "\n",
        "Debido a la forma en que se pasa el estado RNN de un paso a otro, el modelo solo acepta un tamaño de lote fijo una vez construido, asi que tomaremos el pre-entranamiento usando los pesos almacenados y un tamaño de lote de 1. Si se quisiera ejecutar el modelo con un tamaño de lote diferente, se necesitaría reconstruir el modelo y restaurar los pesos desde el punto de control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CmfudwvGsD7y",
        "colab": {}
      },
      "source": [
        "#tomamos un solo batch\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "#utilizamos los checkpoint del pre-entrenamiento\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "#reconstruimos el modelo\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hv0lxIXVsD73",
        "outputId": "a043db67-dc8c-4351-dfa2-344d580ba86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (1, None, 256)            35072     \n",
            "_________________________________________________________________\n",
            "gru_12 (GRU)                 (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (1, None, 1024)           6297600   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 137)            140425    \n",
            "=================================================================\n",
            "Total params: 10,411,401\n",
            "Trainable params: 10,411,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSo6RzBRUtRZ",
        "colab_type": "text"
      },
      "source": [
        "### Predicción\n",
        "\n",
        "¿Cómo se genera el texto?:\n",
        "\n",
        "* Se introduce una cadena de inicio, se inicializa el estado de la  RNN y se configura el número de caracteres a generar.\n",
        "\n",
        "* Se obtiene la distribución de predicción del siguiente carácter utilizando la cadena de inicio y el estado de la  RNN y se calcula el índice del carácter predicho. \n",
        "\n",
        "* Se usa este caracter predicho como la próxima entrada al modelo.\n",
        "\n",
        "* El estado RNN devuelto retroalimenta al modelo para que ahora tenga más contexto, en lugar de una sola palabra. Después de predecir la siguiente palabra, los estados RNN modificados se retroalimentan nuevamente en el modelo, que es cómo aprende a medida que obtiene más contexto de las palabras predichas previamente.\n",
        "\n",
        "Vamos a hacer la función para generar texto basada en los pesos obtenidos en el modelo pre-entrenado, aquí utilizamos un nuevo concepto *la temperatura* que indica que tan sorprendente o aleatorio puede ser un resultado, toma valores entre 0 y 1, donde un valor cercano a 0 indica que es mas predecible y un valor cercano a 1 indica que el resultado es más aleatorio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjLYoVm0sD78",
        "colab": {}
      },
      "source": [
        "#funcion para generar texto usando el modelo pre-entrenado\n",
        "def generate_text(model, start_string, num_generate):\n",
        "\n",
        "  #convertimos el texto inicial en numeros (vectorizacion)\n",
        "  input_eval = [charxid[s] for s in start_string] #vector columna\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #vector fila, agrega una diension\n",
        "\n",
        "  #guardamos el texto predicho\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  #tamaño del bache== 1\n",
        "  model.reset_states() #reinia o borra el estado recurrente de la red. Dejando valores aleatorios o ceros.\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      #removemos la dimensión del batch, quita una dimensión\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      #usamos la distribución categorica para predecir la palabra que retorna el modelo\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      #utilizamos la palabra predicha y el estado oculto anterior como entrada\n",
        "      input_eval = tf.expand_dims([predicted_id], 0) #devuelve un tensor con una dimensión adicional en el eje de índice\n",
        "\n",
        "      text_generated.append(idxchar[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7yknkpUtRp",
        "colab_type": "text"
      },
      "source": [
        "Notemos que el texto generado es capaz de utilizar mayusculas, minusculas y signos de puntuación para construir frases con el vocabulario de los Simpons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1b4ggiAsD8A",
        "outputId": "0ec6ec67-d8c3-4500-f741-3e59f810e29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"Homer Simpson: \", num_generate=500))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Homer Simpson: You're stecking out, but I think they're section of you with some time.\r\n",
            "Bart Simpson: (ANNOYED) Wow! (GIGGLES)\r\n",
            "Lisa Simpson: Fine. But stop supporting a hit to us, that will come triple. This is touched to sested to bright driving and get him? And money to meet straight in!\r\n",
            "David's tear to see until you dedrey from the electric growing.\r\n",
            "Comic Book Guy: Marge, we're going to hear... Hello your store? I've never seen Megical Club cheese, cows... consider... make pupping season...\r\n",
            "Young Well..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OMFMDZLUtR2",
        "colab_type": "text"
      },
      "source": [
        "## Avanzado: Entrenamiento personalizado\n",
        "\n",
        "Para mejorar el entrenamiento anterior y obtener más control usaremos `tf.GradientTape` bajo los siguientes pasos:\n",
        "\n",
        "* Utilizamos el mismo modelo y el optimizador adam.\n",
        "\n",
        "* Inicializamos el estado RNN llamando al método `tf.keras.Model.reset_states`.\n",
        "\n",
        "* Repetimos el conjunto de datos (lote por lote) y calculamos las predicciones asociadas con cada una.\n",
        "\n",
        "* Abrimos un `tf.GradientTape`, y calculamos las predicciones y pérdidas en ese contexto.\n",
        "\n",
        "* Calculamos los gradientes de la pérdida con respecto a las variables del modelo utilizando el método `tf.GradientTape.grads`.\n",
        "\n",
        "* Finalmente, damos un paso hacia abajo utilizando el método `tf.train.Optimizer.apply_gradients` del optimizador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-TP9mJ_3sD8E",
        "colab": {}
      },
      "source": [
        "#modelo\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "78WenvicsD8H",
        "colab": {}
      },
      "source": [
        "#optimizador\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_QI19bNEsD8O",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target): \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgJullsUtST",
        "colab_type": "text"
      },
      "source": [
        "Con lo anterior hacemos el pre-entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "psNF_W9ssD8T",
        "outputId": "5399fc01-f1d8-4d18-f78a-beaddd8215b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  #inicializamos el estado oculto al comienzo de cada época\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target) #usamos la funcion train_step que acabamos de creamr\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  #guardamos los 'checkpoint' del modelo cada 5 epocas\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "#imprimimos la perida y los pesos guardados\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.919189929962158\n",
            "Epoch 1 Batch 100 Loss 2.3017690181732178\n",
            "Epoch 1 Batch 200 Loss 1.8995417356491089\n",
            "Epoch 1 Batch 300 Loss 1.7622944116592407\n",
            "Epoch 1 Batch 400 Loss 1.542039155960083\n",
            "Epoch 1 Batch 500 Loss 1.5134451389312744\n",
            "Epoch 1 Batch 600 Loss 1.4137160778045654\n",
            "Epoch 1 Batch 700 Loss 1.3271125555038452\n",
            "Epoch 1 Batch 800 Loss 1.4190945625305176\n",
            "Epoch 1 Batch 900 Loss 1.3396272659301758\n",
            "Epoch 1 Batch 1000 Loss 1.316850185394287\n",
            "Epoch 1 Batch 1100 Loss 1.3055423498153687\n",
            "Epoch 1 Batch 1200 Loss 1.301363229751587\n",
            "Epoch 1 Batch 1300 Loss 1.191483974456787\n",
            "Epoch 1 Batch 1400 Loss 1.2816455364227295\n",
            "Epoch 1 Batch 1500 Loss 1.224030613899231\n",
            "Epoch 1 Loss 1.2251\n",
            "Time taken for 1 epoch 253.65200638771057 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2686915397644043\n",
            "Epoch 2 Batch 100 Loss 1.2236398458480835\n",
            "Epoch 2 Batch 200 Loss 1.2016856670379639\n",
            "Epoch 2 Batch 300 Loss 1.201305866241455\n",
            "Epoch 2 Batch 400 Loss 1.1975877285003662\n",
            "Epoch 2 Batch 500 Loss 1.1723922491073608\n",
            "Epoch 2 Batch 600 Loss 1.1962389945983887\n",
            "Epoch 2 Batch 700 Loss 1.1604433059692383\n",
            "Epoch 2 Batch 800 Loss 1.2053908109664917\n",
            "Epoch 2 Batch 900 Loss 1.1485878229141235\n",
            "Epoch 2 Batch 1000 Loss 1.1788952350616455\n",
            "Epoch 2 Batch 1100 Loss 1.1812546253204346\n",
            "Epoch 2 Batch 1200 Loss 1.1800377368927002\n",
            "Epoch 2 Batch 1300 Loss 1.1964080333709717\n",
            "Epoch 2 Batch 1400 Loss 1.2310267686843872\n",
            "Epoch 2 Batch 1500 Loss 1.2325868606567383\n",
            "Epoch 2 Loss 1.1354\n",
            "Time taken for 1 epoch 251.1891577243805 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1269125938415527\n",
            "Epoch 3 Batch 100 Loss 1.137258529663086\n",
            "Epoch 3 Batch 200 Loss 1.0596613883972168\n",
            "Epoch 3 Batch 300 Loss 1.1116576194763184\n",
            "Epoch 3 Batch 400 Loss 1.1962889432907104\n",
            "Epoch 3 Batch 500 Loss 1.1785484552383423\n",
            "Epoch 3 Batch 600 Loss 1.1224346160888672\n",
            "Epoch 3 Batch 700 Loss 1.1446843147277832\n",
            "Epoch 3 Batch 800 Loss 1.1491765975952148\n",
            "Epoch 3 Batch 900 Loss 1.112226963043213\n",
            "Epoch 3 Batch 1000 Loss 1.0980335474014282\n",
            "Epoch 3 Batch 1100 Loss 1.1407281160354614\n",
            "Epoch 3 Batch 1200 Loss 1.149395227432251\n",
            "Epoch 3 Batch 1300 Loss 1.1246970891952515\n",
            "Epoch 3 Batch 1400 Loss 1.1132187843322754\n",
            "Epoch 3 Batch 1500 Loss 1.0802546739578247\n",
            "Epoch 3 Loss 1.2127\n",
            "Time taken for 1 epoch 250.73038053512573 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0730597972869873\n",
            "Epoch 4 Batch 100 Loss 1.116891622543335\n",
            "Epoch 4 Batch 200 Loss 1.0742690563201904\n",
            "Epoch 4 Batch 300 Loss 1.1117671728134155\n",
            "Epoch 4 Batch 400 Loss 1.0855135917663574\n",
            "Epoch 4 Batch 500 Loss 1.1158318519592285\n",
            "Epoch 4 Batch 600 Loss 1.092566728591919\n",
            "Epoch 4 Batch 700 Loss 1.1596705913543701\n",
            "Epoch 4 Batch 800 Loss 1.128075361251831\n",
            "Epoch 4 Batch 900 Loss 1.1222724914550781\n",
            "Epoch 4 Batch 1000 Loss 1.0864077806472778\n",
            "Epoch 4 Batch 1100 Loss 1.1103805303573608\n",
            "Epoch 4 Batch 1200 Loss 1.1478437185287476\n",
            "Epoch 4 Batch 1300 Loss 1.1301072835922241\n",
            "Epoch 4 Batch 1400 Loss 1.1203786134719849\n",
            "Epoch 4 Batch 1500 Loss 1.1192736625671387\n",
            "Epoch 4 Loss 1.1097\n",
            "Time taken for 1 epoch 252.12592267990112 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.0469539165496826\n",
            "Epoch 5 Batch 100 Loss 1.061097264289856\n",
            "Epoch 5 Batch 200 Loss 1.1202588081359863\n",
            "Epoch 5 Batch 300 Loss 1.1129121780395508\n",
            "Epoch 5 Batch 400 Loss 1.142590880393982\n",
            "Epoch 5 Batch 500 Loss 1.1618080139160156\n",
            "Epoch 5 Batch 600 Loss 1.1098458766937256\n",
            "Epoch 5 Batch 700 Loss 1.1254377365112305\n",
            "Epoch 5 Batch 800 Loss 1.1070818901062012\n",
            "Epoch 5 Batch 900 Loss 1.0716595649719238\n",
            "Epoch 5 Batch 1000 Loss 1.0658107995986938\n",
            "Epoch 5 Batch 1100 Loss 1.1420327425003052\n",
            "Epoch 5 Batch 1200 Loss 1.1548501253128052\n",
            "Epoch 5 Batch 1300 Loss 1.0827494859695435\n",
            "Epoch 5 Batch 1400 Loss 1.080678939819336\n",
            "Epoch 5 Batch 1500 Loss 1.1123766899108887\n",
            "Epoch 5 Loss 1.0962\n",
            "Time taken for 1 epoch 252.20103192329407 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.0408521890640259\n",
            "Epoch 6 Batch 100 Loss 1.0136024951934814\n",
            "Epoch 6 Batch 200 Loss 1.0463060140609741\n",
            "Epoch 6 Batch 300 Loss 1.1274616718292236\n",
            "Epoch 6 Batch 400 Loss 1.0900483131408691\n",
            "Epoch 6 Batch 500 Loss 1.0659353733062744\n",
            "Epoch 6 Batch 600 Loss 1.0648245811462402\n",
            "Epoch 6 Batch 700 Loss 1.1012470722198486\n",
            "Epoch 6 Batch 800 Loss 1.0887012481689453\n",
            "Epoch 6 Batch 900 Loss 1.1020262241363525\n",
            "Epoch 6 Batch 1000 Loss 1.1314418315887451\n",
            "Epoch 6 Batch 1100 Loss 1.118257761001587\n",
            "Epoch 6 Batch 1200 Loss 1.0196696519851685\n",
            "Epoch 6 Batch 1300 Loss 1.0757997035980225\n",
            "Epoch 6 Batch 1400 Loss 1.095962405204773\n",
            "Epoch 6 Batch 1500 Loss 1.0968942642211914\n",
            "Epoch 6 Loss 1.1063\n",
            "Time taken for 1 epoch 252.62140798568726 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.0296989679336548\n",
            "Epoch 7 Batch 100 Loss 1.0265698432922363\n",
            "Epoch 7 Batch 200 Loss 1.0519249439239502\n",
            "Epoch 7 Batch 300 Loss 1.0361815690994263\n",
            "Epoch 7 Batch 400 Loss 1.09584379196167\n",
            "Epoch 7 Batch 500 Loss 1.0801070928573608\n",
            "Epoch 7 Batch 600 Loss 1.1087311506271362\n",
            "Epoch 7 Batch 700 Loss 1.1241607666015625\n",
            "Epoch 7 Batch 800 Loss 1.0757259130477905\n",
            "Epoch 7 Batch 900 Loss 1.085579752922058\n",
            "Epoch 7 Batch 1000 Loss 1.082304835319519\n",
            "Epoch 7 Batch 1100 Loss 1.085174560546875\n",
            "Epoch 7 Batch 1200 Loss 1.0895333290100098\n",
            "Epoch 7 Batch 1300 Loss 1.1226307153701782\n",
            "Epoch 7 Batch 1400 Loss 1.0994316339492798\n",
            "Epoch 7 Batch 1500 Loss 1.071811556816101\n",
            "Epoch 7 Loss 1.0700\n",
            "Time taken for 1 epoch 253.84775257110596 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0208189487457275\n",
            "Epoch 8 Batch 100 Loss 1.0935114622116089\n",
            "Epoch 8 Batch 200 Loss 1.04887056350708\n",
            "Epoch 8 Batch 300 Loss 1.1016578674316406\n",
            "Epoch 8 Batch 400 Loss 1.0543075799942017\n",
            "Epoch 8 Batch 500 Loss 1.0414959192276\n",
            "Epoch 8 Batch 600 Loss 1.0573006868362427\n",
            "Epoch 8 Batch 700 Loss 1.074755072593689\n",
            "Epoch 8 Batch 800 Loss 1.1069189310073853\n",
            "Epoch 8 Batch 900 Loss 1.0618319511413574\n",
            "Epoch 8 Batch 1000 Loss 1.0604379177093506\n",
            "Epoch 8 Batch 1100 Loss 1.1088569164276123\n",
            "Epoch 8 Batch 1200 Loss 1.0588574409484863\n",
            "Epoch 8 Batch 1300 Loss 1.0734094381332397\n",
            "Epoch 8 Batch 1400 Loss 1.0545426607131958\n",
            "Epoch 8 Batch 1500 Loss 1.0744681358337402\n",
            "Epoch 8 Loss 1.0638\n",
            "Time taken for 1 epoch 253.04330849647522 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9859752058982849\n",
            "Epoch 9 Batch 100 Loss 1.0652202367782593\n",
            "Epoch 9 Batch 200 Loss 1.0610865354537964\n",
            "Epoch 9 Batch 300 Loss 1.0712567567825317\n",
            "Epoch 9 Batch 400 Loss 1.0591236352920532\n",
            "Epoch 9 Batch 500 Loss 1.0945980548858643\n",
            "Epoch 9 Batch 600 Loss 1.0635406970977783\n",
            "Epoch 9 Batch 700 Loss 1.088046669960022\n",
            "Epoch 9 Batch 800 Loss 1.0811635255813599\n",
            "Epoch 9 Batch 900 Loss 1.0739442110061646\n",
            "Epoch 9 Batch 1000 Loss 1.0266095399856567\n",
            "Epoch 9 Batch 1100 Loss 1.093611478805542\n",
            "Epoch 9 Batch 1200 Loss 1.082606554031372\n",
            "Epoch 9 Batch 1300 Loss 1.082651972770691\n",
            "Epoch 9 Batch 1400 Loss 1.0891252756118774\n",
            "Epoch 9 Batch 1500 Loss 1.0301567316055298\n",
            "Epoch 9 Loss 1.0685\n",
            "Time taken for 1 epoch 253.2169008255005 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0544697046279907\n",
            "Epoch 10 Batch 100 Loss 1.0387448072433472\n",
            "Epoch 10 Batch 200 Loss 1.0476702451705933\n",
            "Epoch 10 Batch 300 Loss 1.0700777769088745\n",
            "Epoch 10 Batch 400 Loss 1.1231344938278198\n",
            "Epoch 10 Batch 500 Loss 1.0954619646072388\n",
            "Epoch 10 Batch 600 Loss 1.0796531438827515\n",
            "Epoch 10 Batch 700 Loss 1.0587358474731445\n",
            "Epoch 10 Batch 800 Loss 1.1124252080917358\n",
            "Epoch 10 Batch 900 Loss 1.1398708820343018\n",
            "Epoch 10 Batch 1000 Loss 1.1248966455459595\n",
            "Epoch 10 Batch 1100 Loss 1.0330207347869873\n",
            "Epoch 10 Batch 1200 Loss 1.0372395515441895\n",
            "Epoch 10 Batch 1300 Loss 1.0791596174240112\n",
            "Epoch 10 Batch 1400 Loss 1.1123768091201782\n",
            "Epoch 10 Batch 1500 Loss 1.081121563911438\n",
            "Epoch 10 Loss 1.0685\n",
            "Time taken for 1 epoch 253.18111729621887 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ofV07LtUtSg",
        "colab_type": "text"
      },
      "source": [
        "Ahora, vamos a utilizar el modelo pre-entrenado, pero vamos a utilizar un solo bache."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e0PFxPwIsD8W",
        "colab": {}
      },
      "source": [
        "#tomamos un solo batch\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "#utilizamos los checkpoint del pre-entrenamiento\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "#reconstruimos el modelo\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERw1xFvnUtSs",
        "colab_type": "text"
      },
      "source": [
        "Y utilizamos nuevamente la funcion generadora de texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUpVIHTTUtSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funcion para generar texto usando el modelo pre-entrenado\n",
        "def generate_text(model, start_string, num_generate):\n",
        "\n",
        "  #convertimos el texto inicial en numeros (vectorizacion)\n",
        "  input_eval = [charxid[s] for s in start_string] #vector columna\n",
        "  input_eval = tf.expand_dims(input_eval, 0) #vector fila, agrega una diension\n",
        "\n",
        "  #guardamos el texto predicho\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  #tamaño del bache== 1\n",
        "  model.reset_states() #reinia o borra el estado recurrente de la red. Dejando valores aleatorios o ceros.\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      #removemos la dimensión del batch, quita una dimensión\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      #usamos la distribución categorica para predecir la palabra que retorna el modelo\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      #utilizamos la palabra predicha y el estado oculto anterior como entrada\n",
        "      input_eval = tf.expand_dims([predicted_id], 0) #devuelve un tensor con una dimensión adicional en el eje de índice\n",
        "\n",
        "      text_generated.append(idxchar[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGWFmH-wUtS8",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que este código nos da resultados mas coherentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTUQ4cqMUtS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7175f51b-d2e2-4042-d968-8aad94274741"
      },
      "source": [
        "print(generate_text(model, u\"Hello, how are you? \", 100))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, how are you? Stead that money?\r\n",
            "Maude Flafforms, Krusty's Knny, your boy.\r\n",
            "Homer Simpson: Eh. At least there's an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnig5r37YxgO",
        "colab_type": "text"
      },
      "source": [
        "### Modelos\n",
        "embeding= 256\n",
        "gru= 1024\n",
        "dense= 138\n",
        "dropout= 0.5\n",
        "\n",
        "|embeding|gru|dense|dropout|loss|\n",
        "|::|::|::|::|::|\n",
        "|1|1|1||1.1060|\n",
        "|1|2|1|2|1.0685|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "icqrGCKKAMmZ",
        "colab": {}
      },
      "source": [
        "## 12. Referencias\n",
        "\n",
        "- Aprendizaje profundo, [Repositorio Ciencia-de-datos](https://github.com/AprendizajeProfundo/Ciencia-de-Datos)\n",
        "- Tensorflow, [Generación de texto con una red neuronal recurrente (RNN)](https://www.tensorflow.org/tutorials/text/text_generation)\n",
        "- Google, [Machine learning crash course](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}